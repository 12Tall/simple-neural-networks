知乎上有一篇文章讲，神经网络最主要的用途是分类。那么，神经网络是否可以用来预测一件事情的发展呢？当然应该也是可以的吧！只要计算出来每个维度对应的权重，那么用来做预测不就是计算一个多项式吗？
记得小学时自然课上（或许）学过《寻找共同特征》，大概是最早接触过的分类的教程了。个人理解中，分类中用到的数据，至少是二维的：一个自变量x 表示事物的特征；一个因变量y 表示分类的结果。例如：x>0 表示正数；x<0 表示负数；x=0 非正非负。但是现实中也有很多事物没有明确地分类界线，例如：颜色、形状等。如何（描述）分辨一个橘子是一个橘子？颜色是橘色、圆的但不是球形、酸甜可口、表皮还有一种刺激性气味。颜色、形状、味道、香味，都是我们描述事物的角度，我们看待问题的角度就可以理解成计算机科学中的“维度”。如果给每个角度赋予一定的值，就像空间坐标一样，确定了x,y,x 就确定了空间中一点的位置，那么就行唯一地确定一类事物（吗）。只要我看待事物的角度足够多，就肯定能给它描述清楚了，就像拍照一样：虽然我并不会画画，但是我把镜头捕捉的每一点信息都记录下来，那么得到的结果一定是最真实的。大橘子、小橘子都是橘子不是？最后就是，是不是我们必须每个橘子都尝一口之后才能确定是不是橘子？答案当然是肯定的。但是实际生活中却没人这么做，因为生活中积累的经验告诉我们：只要颜色、形状、气味在合理的范围之内，那么大概率就能确定是橘子。所以对我们人类来说颜色、形状、气味这些维度的重要性就要大于味道这个维度，也就是说存在某些维度的权重大于另一些维度。遗憾的是并不是所有的橘子都具有相同的颜色、形状、味道等特征值，准确地说，我们找不到两个完全相同的橘子，所以我们需要给每个特征值以区间的形式表示出来：只要数值变化在一个差不多的范围内就好了，而实际计算中，对一组特征值（特征向量）进行加权求和之后得到的值，在一个范围内变化中就好了。

综上，在描述（分类）事物的时候，我们至少需要：
1. 一组特征值、一个结果；
2. 每个特征的权重；  
3. 结果的取值范围。

如果以上我们都不知道，那当然是没什么可以做的。假如我们知道了大量的`特征值组合以及分类结果`那么我们就能用机器无限逼近得出每个特征的权重与结果的取值范围，进而对于一组新的特征值、利用已有的权重，就能计算得出事物的分类。

想一想我们高中时，是如何计算平面中一组点的趋势线的：
1. 假设一条直线：y=kx+b
2. 求平面上每个点到该直线的距离
3. 对于所有的距离求和，当和取最小值时，确定k 和b

于是我们就得到了一条最合适的直线。该直线也将平面一分为二，那我们能否根据这种想法对事物进行分类呢？
1. 直线可以将平面一分为二；
2. 平面可以将空间一分为二；
3. n-1 维空间可以将n 维空间一分为二；

对于平面上的两簇点`[x, y]`，我们很容易能得到存在这么一条直线`kx+b=y`将其一分为二：将点代入`kx+b-y`判断最终值的正负。那么如何确定参数`k,b`呢？对于更高维的点`[x0(b), x1, x2, ..., xn, y]`呢？
我们可以先假设一组随机的权重`[w0, w1, w2, ..., wn]`，使用这组权重去乘以`[x0(b), x1, x2, ..., xn]` 就会得到一组预期的结果`[y']`。根据预期与实际的误差`y-y'`，我们可以调整权重，进而使得误差值不断缩小。但是误差如何传递给权重呢？如果我们知道`y` 随`wi`如何变化的就好了。 
1. 正则化处理。正则化处理后，可以比较方便地求出数据变化的斜率和补偿量；  
2. 求斜率、补偿量  
3. 根据斜率求权重补偿量：输入转置乘以补偿量系数，然后累加（要不要求平均？）
4. 更新权重  

不断重复此过程，因为没有涉及到精度，所以需要人为设定循环、迭代次数。于是我们知道了单个神经元的运算过程。



思维需要简短地、自然而然地过渡。

Sigmod 是为了计算概率，梯度下降才是对误差的补偿。
看起来是矩阵的点乘，实际上则是特征向量分别相乘，所以对w 取导数是正确的猜测。